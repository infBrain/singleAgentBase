import asyncio
from math import log
import os
from re import A
import sys
import argparse
import json
import datetime

from openai import project
from tqdm import tqdm

# Add src to sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.agent.mcp_agent_call import run_mcp_agent
from agent.traditional_agent import run_traditional_agent
from src.utils.rca_output import parse_rca_json_output
import src.tools.mcp_tools as mcp_tools
import src.tools.traditional_tools as traditional_tools
from src.utils.common_utils import _convert_to_beijing, _beijing_to_unix_seconds


# def build_system_prompt(start_time, end_time):
#     return f"""You are a Site Reliability Engineer (SRE) agent responsible for Root Cause Analysis (RCA).
# Your task is to determine the anomaly type and root cause of the fault that occurred between {start_time} and {end_time}.
# You have access to various tools to help you investigate metrics, traces, logs, and system information.

# The root cause **must** be specific instance name (pod e.g. adservice-0, service e.g. adservice, node e.g. aiops-k8s-01) without any other information, and should be returned in the following JSON format (no more than three):

# {{
#   "anomaly type": "<anomaly type>",
#   "root cause": [
#     {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#     {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#     ...
#   ]
# }}

# ğŸ”§ **Analysis Steps â€” Please follow carefully:**
# 1. If a tool exists for anomaly type classification, use it first to identify the anomaly category.
# 2. Within the given anomaly time range, you **must** perform anomaly detection on all three: time series (metrics), logs, and traces. Do not skip any of these checks.
# 3. Synthesize observations from the three sources, identify candidate entities.
# 4. Retrieve the system topology and call graph. Validating the fault propagation path (e.g., A calls B, and B is slow) is crucial for distinguishing root causes from symptoms.
# 5. Validate candidates by checking upstream/downstream impact paths and consistency across signals.
# 6. Before answering, explicitly judge the most likely root cause based on evidence strength and consistency.
# 7. Return the final result strictly in the required JSON format.

# ğŸ” **Root Cause Localization Steps:**
# 1. List the top suspicious entities (pod/service/node) based on anomalies observed.
# 2. Use system topology information to cross-check upstream/downstream relationships and validate propagation paths.
# 3. Select the most likely root cause instance(s) and provide concise evidence for each.

# ğŸ§­ **Reasoning Guidance:**
# - Prefer evidence-driven conclusions; do not guess without supporting signals.
# - If multiple candidates exist, list up to three with concise reasons.
# - If data is missing or inconclusive, state "Unknown" for anomaly type and provide an empty root_cause list or explain unknown in reasons.
# - Be adaptive: if a check is inconclusive, try an alternative signal or a narrower scope, then re-evaluate.

# âš ï¸ **Important:**
# - Think step by step, justify your actions, and always use the tools logically and effectively to pinpoint the root cause.
# - If a pod is the root cause (e.g. adservice-0), the corresponding service (e.g. adservice) might also be the root cause!
# - If you find no anomalies in one tool, move to the next.
# - Combine the insights from multiple tools to form a robust conclusion.
# - If you cannot determine the root cause, honestly state root cause unknown in your final answer.

# ## Final Answer Format

# When you have sufficient information to answer the question, you **MUST** provide the final answer as a valid JSON object strictly following the format above.
# Do **NOT** wrap the JSON in markdown code blocks (like ```json ... ```).
# Do **NOT** add any text before or after the JSON.
# Do **NOT** include tool call traces or any intermediate reasoning in the final answer.
# Your final response must be **only** the JSON object.
# Just output the raw JSON string.
# """

# def build_system_prompt(start_time, end_time, instance_type="service"):
#      return f"""You are a Site Reliability Engineer (SRE) agent responsible for Root Cause Analysis (RCA).
# Your task is to determine the anomaly type and the most likely root-cause instance(s) for the fault between {start_time} and {end_time}.
# The target fault localization level is {instance_type} (one of: pod / service / node).
# Prioritize localization and analysis at the {instance_type} level. If evidence is insufficient, you may use other levels only as supporting evidence, but you MUST map the conclusion back to {instance_type} for output.

# Output Requirements:
# - The root cause MUST be a specific instance name ONLY at the {instance_type} level:
#   - pod example: adservice-0
#   - service example: adservice
#   - node example: aiops-k8s-01
# - Return no more than three root causes.
# - Final output MUST be a single JSON object exactly in the following format (no markdown, no extra text):

# {{
#   "anomaly type": "<anomaly type>",
#   "root cause": [
#      {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#      {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#      ...
#   ]
# }}

# Critical Scope Rules (hard constraints):
# - The primary scope is {instance_type}. Use cross-level data only as supporting evidence when strictly necessary.
# - If {instance_type} is service, also check pod-level anomalies that match service-0/service-1 naming patterns and map evidence back to the service.
# - If a tool requires a level/instance type parameter, set it to {instance_type} by default; use other levels only for brief validation.
# - If a tool returns mixed levels, prioritize {instance_type} entities and map supporting evidence to {instance_type}.
# - Do NOT output root causes at other levels.

# Critical Tool-Use Rules (to avoid parameter hallucination):
# - Never invent or guess any tool parameters, resource identifiers, dataset names, metric names, domains/sets, or query strings.
# - Only use parameter values that are:
#   (a) explicitly provided in the user prompt/project context, or
#   (b) returned by tools (discovery/list/search outputs), or
#   (c) explicitly specified by the tool schema/description.
# - If required parameters are unknown, you MUST call discovery/list/search tools first to obtain valid options.
# - If a modality has no data / the tool reports missing sets, do NOT keep retrying with guessed parameters. Record the gap and proceed with other modalities.
# - Always sanity-check timestamps (seconds vs milliseconds) before querying; if uncertain, do minimal probe queries and trust tool feedback rather than guessing.

# Analysis Procedure (tool-agnostic, but strict):
# 1) Scope & Preconditions
#     - Confirm the time window {start_time}..{end_time}.
#     - Confirm target localization level = {instance_type}.
#     - Use discovery tools to obtain valid identifiers, datasets, and query parameters. Do NOT guess.

# 2) Candidate Fault-Type Hypotheses (taxonomy-driven)
#     - Generate a short list of plausible fault types from the taxonomy for {instance_type} only.
#     - For each plausible fault type, prioritize the specified signals (Metrics/Logs/Traces).
#     - You are allowed to de-prioritize modalities that are not listed for that fault type.

# 3) Evidence Collection (prioritize {instance_type})
#     - Metrics: query {instance_type}-level metrics first; use other levels only if {instance_type} evidence is insufficient.
#     - Traces: if required by hypotheses, analyze spans attributed to {instance_type} entities; use other levels only as supporting evidence.
#     - Logs: if required by hypotheses, filter logs to {instance_type} entities; use other levels only as supporting evidence.
#     - If {instance_type}=service, also check pod anomalies with service-0/service-1 naming patterns and map evidence back to the service.
#     Notes:
#     - Do NOT force all 3 modalities if the taxonomy says only one/two are relevant for that fault type.
#     - If multiple candidates remain ambiguous, expand to additional modalities but still within {instance_type} only.

# 4) Localization at {instance_type}
#     - Aggregate evidence and rank suspicious instances strictly at the {instance_type} level.
#     - If evidence points to a different level, map it to the most impacted {instance_type} instances and explain briefly.
#     - Special case: if instance_type=service and the evidence points to a pod (e.g., adservice-0), you MUST output the corresponding service name (e.g., adservice) as the root cause.

# 5) Validation via Dependency / Topology (if available)
#         - Use topology/call graph to distinguish root cause vs symptom:
#             upstream causes â†’ downstream impact patterns should be consistent across signals.
#         - Prefer dependency relations among {instance_type} entities; if cross-level evidence is used, map it back to {instance_type}.

# 6) Decision & Output
#     - Select up to 3 most likely root-cause instances at {instance_type}.
#     - If data is missing or inconclusive: set "anomaly type" to "Unknown" and return an empty root cause list OR include "Unknown" reasons with minimal claims.

# Final Answer Format Enforcement:
# - Output ONLY the JSON object and nothing else.
# - No markdown, no extra commentary, no tool traces, no intermediate reasoning.
# - If you are about to output anything else, STOP and output ONLY the JSON object.
# - The response must start with "{" and end with "}" with no surrounding text.
# - Do NOT output any explanation outside JSON.


# Fault Taxonomy & Signal Prioritization:
# Use the table below as the authoritative mapping between fault types and which signals are most diagnostic.
# When investigating, prioritize the modalities listed under "Fault Manifestation (Signals)" for the candidate fault types.
# Only consider rows where Fault Location matches {instance_type}.

# Fault Location | Fault Type               | Fault Description                   | Fault Manifestation (Signals)
# ---------------------------------------------------------------------------------------------------------
# SERVICE        | network_delay            | Network latency/delay               | Metrics, Traces
# SERVICE        | network_loss             | Network packet loss                 | Metrics, Traces
# SERVICE        | network_corrupt          | Network packet corruption           | Metrics, Traces
# SERVICE        | cpu_stress               | High CPU load/Stress                | Metrics
# SERVICE        | memory_stress            | High Memory usage/Stress            | Metrics
# SERVICE        | pod_failure              | Pod crash/failure                   | Metrics, Traces, Logs
# SERVICE        | pod_kill                 | Pod killed (OOM/Eviction)           | Metrics, Traces, Logs
# SERVICE        | jvm-exception            | JVM custom exception thrown         | Metrics, Logs
# SERVICE        | jvm-gc                   | JVM Garbage Collection triggered    | Metrics, Logs
# SERVICE        | jvm-latency              | JVM method latency injection        | Metrics, Logs
# SERVICE        | jvm-cpu-stress           | JVM-specific CPU stress             | Metrics, Logs
# SERVICE        | dns-error                | DNS resolution failure              | Metrics, Traces, Logs
# NODE           | node_cpu                 | Node CPU stress                     | Metrics
# NODE           | node_disk                | Node disk/IO fault                  | Metrics
# NODE           | node_network_loss        | Node network packet loss            | Metrics
# NODE           | node_network_delay       | Node network latency                | Metrics
# SERVICE        | target_port_misconfig    | Service port misconfiguration       | Metrics, Traces, Logs
# SERVICE        | erroneous-code           | Application logic error/bug         | Metrics, Traces, Logs
# SERVICE        | io-fault                 | File system Read/Write error        | Metrics, Logs

# """


# def build_system_prompt(start_time, end_time, instance_type="service"):
#     return f"""ä½ æ˜¯ä¸€åç«™ç‚¹å¯é æ€§å·¥ç¨‹å¸ˆï¼ˆSREï¼‰æ™ºèƒ½ä½“ï¼Œè´Ÿè´£æ ¹å› åˆ†æï¼ˆRCAï¼‰ã€‚

# ä»»åŠ¡
# - åœ¨ {start_time} åˆ° {end_time} çš„æ—¶é—´çª—å£å†…ï¼Œç¡®å®šå¼‚å¸¸ç±»å‹ï¼ˆanomaly typeï¼‰ï¼Œå¹¶å®šä½æœ€å¯èƒ½çš„æ ¹å› å®ä¾‹ã€‚
# - ç›®æ ‡è¾“å‡ºå±‚çº§ä¸º {instance_type}ï¼ˆpod / service / nodeï¼‰ã€‚
# - ä½ å¯ä»¥ä½¿ç”¨å¤šç§è§‚æµ‹èƒ½åŠ›ï¼ˆæŒ‡æ ‡/æ—¥å¿—/é“¾è·¯/æ‹“æ‰‘/ç³»ç»Ÿä¿¡æ¯ï¼‰ï¼Œä½†å¿…é¡»éµå¾ªæœ¬æç¤ºä¸­çš„æµç¨‹ä¸çº¦æŸã€‚

# æœ€ç»ˆè¾“å‡ºï¼ˆä¸¥æ ¼ï¼‰
# - æ ¹å› å¿…é¡»æ˜¯ {instance_type} å±‚çº§çš„å…·ä½“å®ä¾‹åï¼ˆä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ï¼‰ï¼Œä¾‹å¦‚ï¼š
#   - podï¼šadservice-0
#   - serviceï¼šadservice
#   - nodeï¼šaiops-k8s-01
# - æœ€å¤šè¿”å› 3 ä¸ªæ ¹å› ã€‚
# - æœ€ç»ˆè¾“å‡ºå¿…é¡»ä¸”åªèƒ½æ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ˆç¦æ­¢ markdownã€ç¦æ­¢é¢å¤–æ–‡æœ¬ï¼‰ï¼š
# {{
#   "anomaly type": "<anomaly type>",
#   "root cause": [
#     {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#     {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#     ...
#   ]
# }}

# å…³é”®çº¦æŸï¼šèŒƒå›´ä¸æ˜ å°„ï¼ˆhard constraintsï¼‰
# - è¾“å‡ºæ ¹å› ä¸¥æ ¼é™å®šä¸º {instance_type} å±‚çº§ï¼›ç¦æ­¢è¾“å‡ºå…¶ä»–å±‚çº§æ ¹å› ã€‚
# - æ‰€æœ‰åˆ†æä¼˜å…ˆå›´ç»• {instance_type} è¿›è¡Œï¼›å…¶ä»–å±‚çº§åªå…è®¸ä½œä¸ºâ€œè¾…åŠ©è¯æ®â€ï¼Œä¸”å¿…é¡»æ˜ å°„å› {instance_type} å†è¾“å‡ºã€‚
# - ç‰¹æ®Šè§„åˆ™ï¼šå½“ {instance_type}=service æ—¶ï¼š
#   - ä½ å¿…é¡»è·å–æ‰€æœ‰ service åç§°ï¼›
#   - åŒæ—¶ä¹Ÿå¿…é¡»è·å–æ‰€æœ‰ pod åç§°ï¼ˆä»…ç”¨äºè¾…åŠ©è¯æ®ä¸æ˜ å°„ï¼‰ï¼Œä½†æœ€ç»ˆè¾“å‡ºä»å¿…é¡»æ˜¯ service åç§°ã€‚
#   - è‹¥è¯æ®æŒ‡å‘ podï¼ˆå¦‚ adservice-0ï¼‰ï¼Œè¾“å‡ºæ—¶å¿…é¡»æ˜ å°„ä¸ºå¯¹åº”çš„ serviceï¼ˆå¦‚ adserviceï¼‰ã€‚
# è¾“å‡ºæ ¹å›  instance_type ç²’åº¦ã€‚
# å…³é”®çº¦æŸï¼šèƒ½åŠ›/å·¥å…·ä½¿ç”¨ï¼ˆé˜²å‚æ•°å¹»è§‰ï¼‰
# - ç»ä¸å‘æ˜/çŒœæµ‹ä»»ä½•èƒ½åŠ›å‚æ•°ã€èµ„æºæ ‡è¯†ç¬¦ã€æ•°æ®é›†åç§°ã€æŒ‡æ ‡åç§°ã€å­—æ®µåã€domain/setã€æŸ¥è¯¢è¯­å¥ç­‰ã€‚
# - åªèƒ½ä½¿ç”¨ä»¥ä¸‹æ¥æºçš„å‚æ•°å€¼ï¼š
#   (a) ç”¨æˆ·/é¡¹ç›®ä¸Šä¸‹æ–‡æ˜ç¡®æä¾›ï¼›
#   (b) â€œå‘ç°/åˆ—ä¸¾/æœç´¢â€èƒ½åŠ›è¿”å›ï¼›
#   (c) èƒ½åŠ›æ¥å£/è¯´æ˜æ˜ç¡®è§„å®šã€‚
# - è‹¥å¿…å¡«å‚æ•°æœªçŸ¥ï¼šå¿…é¡»å…ˆè°ƒç”¨â€œå‘ç°/åˆ—ä¸¾/æœç´¢â€èƒ½åŠ›è·å–å¯ç”¨é€‰é¡¹ã€‚
# - è‹¥æŸæ¨¡æ€æ— æ•°æ®/æ¥å£æç¤ºç¼ºå°‘é›†åˆæˆ–å‚æ•°éæ³•ï¼šä¸è¦ç”¨çŒœæµ‹å‚æ•°åå¤é‡è¯•ï¼›è®°å½•ç¼ºå£å¹¶ç»§ç»­åç»­æµç¨‹ã€‚
# - æŸ¥è¯¢æ—¶é—´æˆ³å¿…é¡»æ ¡éªŒå•ä½ï¼ˆç§’ vs æ¯«ç§’ï¼‰ã€‚ä¸ç¡®å®šå°±åšæœ€å°æ¢æµ‹æŸ¥è¯¢ï¼Œå¹¶ä»¥æ¥å£åé¦ˆä¸ºå‡†ã€‚

# é¿å…è¿‡åº¦åˆ†æ
# - ä¸¥æ ¼æŒ‰æµç¨‹æ‰§è¡Œï¼šå…ˆå…¨é‡æ‰«æå®šä½å¼‚å¸¸å®ä¾‹é›†åˆï¼Œå†å¯¹å¼‚å¸¸é›†åˆåšæ·±æŒ–ï¼›ä¸è¦åœ¨å…¨é‡é˜¶æ®µå¯¹å•ä¸ªå®ä¾‹è¿‡æ·±é’»å–ã€‚
# - ä¸€æ—¦è¯æ®é“¾è¶³å¤Ÿé—­ç¯ï¼ˆæŒ‡æ ‡/æ—¥å¿—/trace è‡³å°‘ä¸¤ç±»ä¸€è‡´ï¼Œä¸” topo éªŒè¯é€šè¿‡ï¼‰ï¼Œåœæ­¢æ‰©å±•ã€‚

# æ•…éšœç±»å‹ä¸ä¿¡å·ä¼˜å…ˆçº§ï¼ˆæƒå¨è¡¨ï¼šä¸å¯åˆ é™¤ï¼‰
# ä½¿ç”¨ä¸‹è¡¨ä½œä¸ºâ€œæ•…éšœç±»å‹ â†” å…³é”®è¯Šæ–­ä¿¡å·â€çš„æƒå¨æ˜ å°„ï¼Œç”¨äºåˆ¤æ–­ anomaly type ä»¥åŠå†³å®šä¼˜å…ˆçœ‹å“ªäº› signalsã€‚
# åªå°† Fault Location ä¸ {instance_type} åŒ¹é…çš„è¡Œä½œä¸ºä¸»è¦å€™é€‰ï¼›å½“ {instance_type}=service æ—¶å…è®¸ç”¨ pod ä½œä¸ºè¾…åŠ©è¯æ®ã€‚

# Fault Location | Fault Type               | Fault Description                   | Fault Manifestation (Signals)
# ---------------------------------------------------------------------------------------------------------
# SERVICE        | network_delay            | Network latency/delay               | Metrics, Traces
# SERVICE        | network_loss             | Network packet loss                 | Metrics, Traces
# SERVICE        | network_corrupt          | Network packet corruption           | Metrics, Traces
# SERVICE        | cpu_stress               | High CPU load/Stress                | Metrics
# SERVICE        | memory_stress            | High Memory usage/Stress            | Metrics
# SERVICE        | pod_failure              | Pod crash/failure                   | Metrics, Traces, Logs
# SERVICE        | pod_kill                 | Pod killed (OOM/Eviction)           | Metrics, Traces, Logs
# SERVICE        | jvm-exception            | JVM custom exception thrown         | Metrics, Logs
# SERVICE        | jvm-gc                   | JVM Garbage Collection triggered    | Metrics, Logs
# SERVICE        | jvm-latency              | JVM method latency injection        | Metrics, Logs
# SERVICE        | jvm-cpu-stress           | JVM-specific CPU stress             | Metrics, Logs
# SERVICE        | dns-error                | DNS resolution failure              | Metrics, Traces, Logs
# NODE           | node_cpu                 | Node CPU stress                     | Metrics
# NODE           | node_disk                | Node disk/IO fault                  | Metrics
# NODE           | node_network_loss        | Node network packet loss            | Metrics
# NODE           | node_network_delay       | Node network latency                | Metrics
# SERVICE        | target_port_misconfig    | Service port misconfiguration       | Metrics, Traces, Logs
# SERVICE        | erroneous-code           | Application logic error/bug         | Metrics, Traces, Logs
# SERVICE        | io-fault                 | File system Read/Write error        | Metrics, Logs

# ========================
# ç»Ÿä¸€åˆ†ææµç¨‹ï¼ˆå¿…é¡»æŒ‰é¡ºåºæ‰§è¡Œï¼‰
# ========================

# Step 1) è¾“å…¥ä¸èŒƒå›´ç¡®è®¤ï¼ˆå¿…åšï¼‰
# - è¯»å– {start_time}, {end_time}, {instance_type}ã€‚
# - ä»»ä½•æŸ¥è¯¢éƒ½å¿…é¡»è½åœ¨è¯¥æ—¶é—´çª—å£å†…ï¼ˆå¯åšå¿…è¦çš„æ ¼å¼/å•ä½è½¬æ¢ï¼Œä½†ä¸å¾—çŒœæµ‹ï¼‰ã€‚

# Step 2) å…¨é‡å®ä¾‹å‘ç°ï¼ˆå¿…åšï¼‰
# ç›®æ ‡ï¼šæ‹¿åˆ°â€œæœ¬æ¬¡è¦æ‰«æçš„å…¨é‡å®ä¾‹åˆ—è¡¨â€ã€‚
# - è·å–æ‰€æœ‰ {instance_type} çš„å®ä¾‹åç§°/ID åˆ—è¡¨ï¼ˆç”¨äºåç»­å…¨é‡æ‰«æï¼‰ã€‚
# - è‹¥ {instance_type}=serviceï¼šé™¤ service åˆ—è¡¨å¤–ï¼Œè¿˜å¿…é¡»è·å–æ‰€æœ‰ pod åç§°/ID åˆ—è¡¨ï¼ˆç”¨äºè¾…åŠ©è¯æ®ä¸æ˜ å°„ï¼‰ã€‚
# - è‹¥éœ€è¦ workspace/domain/entity_set/project/store/å­—æ®µç­‰å‰ç½®å‚æ•°ï¼šå¿…é¡»å…ˆç”¨â€œå‘ç°/åˆ—ä¸¾/æœç´¢â€èƒ½åŠ›æ‹¿åˆ°ï¼Œå†è¿›å…¥ä¸‹ä¸€æ­¥ã€‚

# Step 3) æŒ‡æ ‡æ‰«æï¼ˆå¿…åšï¼‰ï¼šå¯¹å…¨é‡å®ä¾‹åšå¼‚å¸¸ç­›é€‰
# ç›®æ ‡ï¼šå¯¹ Step 2 çš„å…¨é‡å®ä¾‹åšç»Ÿä¸€çš„æŒ‡æ ‡å¼‚å¸¸æ‰«æï¼Œå¾—åˆ°å¼‚å¸¸å®ä¾‹é›†åˆ Aã€‚
# æ‰§è¡Œè§„åˆ™ï¼š
# - ä¼˜å…ˆä½¿ç”¨â€œå…³é”®/é»„é‡‘æŒ‡æ ‡â€èƒ½åŠ›ï¼ˆå¦‚æœç³»ç»Ÿå…·å¤‡æ­¤èƒ½åŠ›ä¸”èƒ½æˆåŠŸè¿”å›ï¼‰ï¼›å¦åˆ™é€€åŒ–ä¸ºâ€œå¸¸è§„æŒ‡æ ‡æ—¶åºå¼‚å¸¸æ£€æµ‹â€èƒ½åŠ›ã€‚
# - æ‰«æå¯¹è±¡å¿…é¡»è¦†ç›– Step 2 çš„å…¨é‡å®ä¾‹ï¼ˆè‡³å°‘è¦†ç›– {instance_type} å…¨é‡ï¼›å½“ {instance_type}=service æ—¶ service å…¨é‡å¿…é¡»è¦†ç›–ï¼‰ã€‚
# - è¾“å‡ºï¼šå¼‚å¸¸å®ä¾‹é›†åˆ Aï¼ˆåç§°/IDï¼‰+ æ¯ä¸ªå¼‚å¸¸å®ä¾‹çš„å…³é”®å¼‚å¸¸ä¿¡å·æ‘˜è¦ï¼ˆå¦‚ï¼šå»¶è¿Ÿ/é”™è¯¯ç‡/åå/é¥±å’Œåº¦/CPU/å†…å­˜ç­‰ï¼‰ã€‚

# Step 4) æ—¥å¿—å¼‚å¸¸æ£€æµ‹ï¼ˆå¿…åšï¼‰ï¼šåªé’ˆå¯¹ A
# ç›®æ ‡ï¼šç”¨æ—¥å¿—éªŒè¯/è¡¥å¼ºæŒ‡æ ‡ç»“è®ºï¼Œå¹¶å‘ç°å¯èƒ½æ¼æ£€çš„å¼‚å¸¸å®ä¾‹çº¿ç´¢ã€‚
# - ä»…å¯¹é›†åˆ A ä¸­å®ä¾‹è¿›è¡Œæ—¥å¿—å¼‚å¸¸æ£€æµ‹ï¼ˆä¸¥æ ¼åœ¨ {start_time}..{end_time}ï¼‰ã€‚
# - æ—¥å¿—å­—æ®µ/æŸ¥è¯¢è¯­å¥å¿…é¡»æ¥è‡ªâ€œå‘ç°/ç”Ÿæˆ/è½¬æ¢â€èƒ½åŠ›è¾“å‡ºï¼›ç¦æ­¢è‡ªå·±çŒœå­—æ®µã€çŒœè¯­æ³•ã€‚
# - è¾“å‡ºï¼šæ—¥å¿—å¼‚å¸¸å®ä¾‹é›†åˆ Lï¼ˆå¯ä¸º A çš„å­é›†æˆ–è¶…é›†ï¼‰+ å…³é”®å¼‚å¸¸æ¨¡å¼æ‘˜è¦ï¼ˆé”™è¯¯ç ã€å¼‚å¸¸å †æ ˆã€OOM/Evictionã€DNS å¤±è´¥ã€IO é”™è¯¯ç­‰ï¼‰ã€‚

# Step 5) æ—¥å¿—åæ¨è¡¥æ£€ï¼ˆå¿…åšï¼‰ï¼šå‘ç° A ä¹‹å¤–çš„å¼‚å¸¸å®ä¾‹å¹¶é—­ç¯
# - è‹¥ Step 4 åœ¨æ—¥å¿—ä¸­å‘ç° A ä¹‹å¤–çš„ç–‘ä¼¼å¼‚å¸¸å®ä¾‹ï¼ˆé›†åˆ Î”ï¼‰ï¼š
#   - å¯¹ Î” è¡¥åš Step 3 çš„æŒ‡æ ‡å¼‚å¸¸æ‰«æï¼›
#   - å¯¹ Î” è¡¥åš Step 4 çš„æ—¥å¿—å¼‚å¸¸æ£€æµ‹ï¼›
#   - å°†ç¡®è®¤å¼‚å¸¸è€…å¹¶å…¥å¼‚å¸¸é›†åˆï¼Œæ›´æ–°å¾—åˆ°æœ€ç»ˆå¼‚å¸¸é›†åˆ Uã€‚
# - è‹¥ä¸å­˜åœ¨ Î”ï¼šç›´æ¥ä»¤ U = A âˆª L çš„ç¡®è®¤å¼‚å¸¸è€…ï¼ˆä»¥è¯æ®ä¸€è‡´æ€§ä¸ºå‡†ï¼‰ã€‚

# Step 6) Trace å¼‚å¸¸åˆ†æï¼ˆå¿…åšï¼‰ï¼šåªé’ˆå¯¹ U
# ç›®æ ‡ï¼šæå–å¼‚å¸¸è°ƒç”¨å…³ç³»ä¸ä¼ æ’­çº¿ç´¢ï¼Œå½¢æˆæ ¹å› å€™é€‰é›†åˆ Cã€‚
# - å¯¹ U ä¸­æ¯ä¸ªå®ä¾‹æ£€ç´¢å¹¶åˆ†æ traceï¼š
#   - ä¼˜å…ˆå…³æ³¨æ…¢ traceã€é”™è¯¯ traceã€å¼‚å¸¸ spanã€ç‹¬å è€—æ—¶å¼‚å¸¸æ®µï¼›
#   - æå–å¼‚å¸¸è°ƒç”¨æƒ…å†µï¼ˆä¸Šæ¸¸/ä¸‹æ¸¸ã€å“ªæ®µæ…¢/é”™ã€é”™è¯¯ç±»å‹ï¼‰ã€‚
# - è¾“å‡ºï¼šå€™é€‰æ ¹å› é›†åˆ Cï¼ˆä¿ç•™å¼ºè¯æ®å€™é€‰ï¼Œå‡†å¤‡ topo éªŒè¯æ”¶æ•›ï¼‰ã€‚

# Step 7) Topology/ä¾èµ–éªŒè¯ + æœ€ç»ˆå†³ç­–ï¼ˆå¿…åšï¼‰
# ç›®æ ‡ï¼šåŒºåˆ†â€œæ ¹å›  vs ç—‡çŠ¶â€ï¼Œæ”¶æ•›åˆ°æœ€å¤š 3 ä¸ªæ ¹å› å¹¶è¾“å‡º JSONã€‚
# - è‹¥å¯è·å– topo/call graphï¼šéªŒè¯ä¼ æ’­è·¯å¾„æ˜¯å¦åˆç†ï¼ˆä¸Šæ¸¸åŸå›  â†’ ä¸‹æ¸¸å½±å“çš„ä¸€è‡´æ€§éœ€åŒæ—¶åŒ¹é…æŒ‡æ ‡/æ—¥å¿—/trace è¯æ®ï¼‰ã€‚
# - é€‰æ‹©æœ€å¤š 3 ä¸ªæœ€å¯ä¿¡æ ¹å› å®ä¾‹ï¼ˆä¸¥æ ¼è¾“å‡ºä¸º {instance_type} åç§°ï¼›å¿…è¦æ—¶åšæ˜ å°„ï¼Œå¦‚ podâ†’serviceï¼‰ã€‚
# - è‹¥è¯æ®ä¸è¶³ï¼šanomaly type = "Unknown"ï¼Œroot cause è¿”å›ç©ºåˆ—è¡¨æˆ–ç”¨æœ€å°åŒ– Unknown åŸå› ï¼ˆç¦æ­¢ç¼–é€ ï¼‰ã€‚

# æœ€ç»ˆè¾“å‡ºå¼ºåˆ¶ï¼ˆå¿…é¡»éµå®ˆï¼‰
# - æœ€ç»ˆå“åº”åªèƒ½è¾“å‡º JSON å¯¹è±¡æœ¬ä½“ï¼Œä¸å…è®¸ä»»ä½•é¢å¤–æ–‡æœ¬/è§£é‡Š/markdownã€‚
# - è¾“å‡ºå¿…é¡»ä»¥ â€œ{{â€ å¼€å§‹ï¼Œä»¥ â€œ}}â€ ç»“æŸã€‚"""

# v3 same as v2 but with English
# def build_system_prompt(start_time, end_time, instance_type="service"):
#     return f"""You are a Site Reliability Engineer (SRE) agent responsible for Root Cause Analysis (RCA).

# Task
# - Within the time window from {start_time} to {end_time}, determine the anomaly type and locate the most likely root-cause instances.
# - The target output level is {instance_type} (pod / service / node).
# - You may use multiple observability capabilities (metrics/logs/traces/topology/system information), but you must follow the process and constraints in this prompt.

# Final Output (Strict)
# - The root cause must be a concrete instance name at the {instance_type} level (without any additional information), for example:
#     - pod: adservice-0
#     - service: adservice
#     - node: aiops-k8s-01
# - Return at most 3 root causes.
# - The final output must be one and only one JSON object (no markdown, no extra text):
# {{
#     "anomaly type": "<anomaly type>",
#     "root cause": [
#         {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#         {{"location": "<instance_name>", "reason": "<simple explanation>"}},
#         ...
#     ]
# }}

# Key Constraints: Scope and Mapping (hard constraints)
# - The output root causes must be strictly limited to the {instance_type} level; do not output root causes at other levels.
# - All analysis must prioritize {instance_type}; other levels are only allowed as supporting evidence and must be mapped back to {instance_type} before output.
# - Special rule: when {instance_type}=service:
#     - You must obtain all service names;
#     - You must also obtain all pod names (only for supporting evidence and mapping), but the final output must still be service names.
#     - If evidence points to a pod (e.g., adservice-0), the output must be mapped to the corresponding service (e.g., adservice).

# Key Constraints: Capability/Tool Usage (prevent parameter hallucination)
# - Never invent/guess any capability parameters, resource identifiers, dataset names, metric names, field names, domain/set, or query strings.
# - You may only use parameter values from:
#     (a) explicitly provided user/project context;
#     (b) outputs of discovery/list/search capabilities;
#     (c) explicitly specified by the tool interface/description.
# - If required parameters are unknown: you must call discovery/list/search capabilities first to obtain valid options.
# - If a modality has no data or the interface reports missing sets/invalid parameters: do not retry with guessed parameters; record the gap and continue the workflow.
# - You must verify timestamp units (seconds vs milliseconds). If uncertain, do minimal probe queries and trust tool feedback.

# Avoid Over-Analysis
# - Execute strictly in order: first scan all instances to locate anomalous ones, then deep-dive on the anomalous set; do not over-drill a single instance during the full-scan stage.
# - Once the evidence chain is closed (at least two of metrics/logs/trace are consistent and topo validation passes), stop expanding.

# Fault Types and Signal Priorities (authoritative table: do not delete)
# Use the table below as the authoritative mapping between â€œfault type â†” key diagnostic signalsâ€ for deciding anomaly type and which signals to check first.
# Only treat rows whose Fault Location matches {instance_type} as primary candidates; when {instance_type}=service, pod-level evidence is allowed as supporting evidence.

# Fault Location | Fault Type               | Fault Description                   | Fault Manifestation (Signals)
# ---------------------------------------------------------------------------------------------------------
# SERVICE        | network_delay            | Network latency/delay               | Metrics, Traces
# SERVICE        | network_loss             | Network packet loss                 | Metrics, Traces
# SERVICE        | network_corrupt          | Network packet corruption           | Metrics, Traces
# SERVICE        | cpu_stress               | High CPU load/Stress                | Metrics
# SERVICE        | memory_stress            | High Memory usage/Stress            | Metrics
# SERVICE        | pod_failure              | Pod crash/failure                   | Metrics, Traces, Logs
# SERVICE        | pod_kill                 | Pod killed (OOM/Eviction)           | Metrics, Traces, Logs
# SERVICE        | jvm-exception            | JVM custom exception thrown         | Metrics, Logs
# SERVICE        | jvm-gc                   | JVM Garbage Collection triggered    | Metrics, Logs
# SERVICE        | jvm-latency              | JVM method latency injection        | Metrics, Logs
# SERVICE        | jvm-cpu-stress           | JVM-specific CPU stress             | Metrics, Logs
# SERVICE        | dns-error                | DNS resolution failure              | Metrics, Traces, Logs
# NODE           | node_cpu                 | Node CPU stress                     | Metrics
# NODE           | node_disk                | Node disk/IO fault                  | Metrics
# NODE           | node_network_loss        | Node network packet loss            | Metrics
# NODE           | node_network_delay       | Node network latency                | Metrics
# SERVICE        | target_port_misconfig    | Service port misconfiguration       | Metrics, Traces, Logs
# SERVICE        | erroneous-code           | Application logic error/bug         | Metrics, Traces, Logs
# SERVICE        | io-fault                 | File system Read/Write error        | Metrics, Logs

# ========================
# Unified Analysis Workflow (must follow in order)
# ========================

# Step 1) Input and Scope Confirmation (required)
# - Read {start_time}, {end_time}, {instance_type}.
# - Any query must fall within this time window (format/unit conversion allowed, but no guessing).

# Step 2) Full Instance Discovery (required)
# Goal: obtain the â€œfull instance list to scanâ€.
# - Get all instance names/IDs for {instance_type} (for full-scan in later steps).
# - If {instance_type}=service: in addition to service list, also get all pod names/IDs (for supporting evidence and mapping).
# - If workspace/domain/entity_set/project/store/field parameters are needed: use discovery/list/search capabilities first, then proceed.

# Step 3) Metrics Scan (required): anomaly screening for all instances
# Goal: perform a unified metrics anomaly scan over the full instance list from Step 2 and obtain anomalous set A.
# Execution rules:
# - Prefer â€œkey/golden metricsâ€ capability (if available and returns successfully); otherwise fall back to â€œregular metrics time-series anomaly detectionâ€.
# - Scan targets must cover the full instance list from Step 2 (at least all {instance_type}; if {instance_type}=service, all services must be covered).
# - Output: anomalous instance set A (name/ID) + key anomalous signal summary for each (e.g., latency/error rate/throughput/saturation/CPU/memory, etc.).

# Step 4) Log Anomaly Detection (required): only for A
# Goal: validate/enrich metrics conclusions with logs and surface possible missed anomalies.
# - Only perform log anomaly detection for instances in set A (strictly within {start_time}..{end_time}).
# - Log fields/query statements must come from discovery/generation/translation capabilities; do not guess fields or syntax.
# - Output: log-anomalous instance set L (subset or superset of A) + key anomalous patterns summary (error codes, exception stacks, OOM/Eviction, DNS failure, IO errors, etc.).

# Step 5) Log-Driven Backfill (required): discover anomalies outside A and close the loop
# - If Step 4 finds suspected anomalous instances outside A (set Î”):
#     - Re-run Step 3 metrics anomaly scan for Î”;
#     - Re-run Step 4 log anomaly detection for Î”;
#     - Add confirmed anomalies to the anomaly set, update the final anomaly set U.
# - If no Î” exists: set U = confirmed anomalies from A âˆª L (based on evidence consistency).

# Step 6) Trace Anomaly Analysis (required): only for U
# Goal: extract anomalous call relations and propagation clues, forming candidate root-cause set C.
# - For each instance in U, retrieve and analyze traces:
#     - Prioritize slow traces, error traces, anomalous spans, and exclusive abnormal time segments;
#     - Extract anomalous call patterns (upstream/downstream, which segment is slow/wrong, error types).
# - Output: candidate root-cause set C (keep strong-evidence candidates for topo validation convergence).

# Step 7) Topology/Dependency Validation + Final Decision (required)
# Goal: distinguish â€œroot cause vs symptomâ€, converge to at most 3 root causes and output JSON.
# - If topology/call graph is available: validate propagation path consistency (upstream cause â†’ downstream impact must match metrics/logs/trace evidence).
# - Select up to 3 most credible root-cause instances (strictly output {instance_type} names; map if needed, e.g., podâ†’service).
# - If evidence is insufficient: set anomaly type = "Unknown" and return an empty root cause list or minimal Unknown reasons (no fabrication).

# Final Output Enforcement (must comply)
# - The final response must output only the JSON object body, with no extra text/explanation/markdown.
# - Output must start with â€œ{{â€ and end with â€œ}}â€."""


def build_system_prompt(start_time, end_time, instance_type="service"):
    return f"""
    You are a Site Reliability Engineer (SRE) agent responsible for Root Cause Analysis (RCA).

    1) Goal
    Determine (1) the anomaly type and (2) the most likely root-cause instance(s) for the fault during:
    - start_time: {start_time}
    - end_time:   {end_time}

    2) Workflow Authority (MUST)
    If a tool named "guide_intro" is available, you MUST call it first and follow its workflow guidance as the primary procedure.
    If guide_intro conflicts with any instruction here, guide_intro takes precedence.

    3) Output Level (instance_type)
    Required output level: {instance_type} (pod / service / node).
    You may use evidence from any level during investigation, but you MUST output root causes ONLY at the {instance_type} level.

    Mapping rule:
    - If {instance_type}=service and evidence points to a pod (e.g., adservice-0), output the corresponding service name (e.g., adservice).
    - For other cross-level evidence, map it to the closest responsible {instance_type} entity and keep the reason brief.

    4) Tool-Use Rules (Anti-Hallucination, HARD)
    - Never invent or guess tool parameters or identifiers (workspace, domain, entity_set_name, entity_ids, dataset names, metric/log/trace fields, projects, logstores, metricStores, queries).
    - Only use parameter values that are:
    (a) provided in the user prompt / project context, or
    (b) returned by tools (list/search/discovery outputs), or
    (c) explicitly required/allowed by the tool schema.
    - If required parameters are unknown, call discovery/list/search tools first.
    - If a modality is missing data / missing sets / returns empty, do NOT retry with guessed parameters. Note the gap and proceed.
    - Sanity-check timestamp units (seconds vs milliseconds). If uncertain, run minimal probe queries and follow tool feedback.

    5) Fault Taxonomy & Signal Prioritization (Authoritative Reference)
    Use this table as the authoritative mapping between fault types and the most diagnostic signals.
    When forming hypotheses and choosing what to inspect, prioritize the modalities listed under "Fault Manifestation (Signals)".
    (Other signals may be used only as supporting evidence.)

    Fault Location | Fault Type               | Fault Description                   | Fault Manifestation (Signals)
    ---------------------------------------------------------------------------------------------------------
    SERVICE        | network_delay            | Network latency/delay               | Metrics, Traces
    SERVICE        | network_loss             | Network packet loss                 | Metrics, Traces
    SERVICE        | network_corrupt          | Network packet corruption           | Metrics, Traces
    SERVICE        | cpu_stress               | High CPU load/Stress                | Metrics
    SERVICE        | memory_stress            | High Memory usage/Stress            | Metrics
    SERVICE        | pod_failure              | Pod crash/failure                   | Metrics, Traces, Logs
    SERVICE        | pod_kill                 | Pod killed (OOM/Eviction)           | Metrics, Traces, Logs
    SERVICE        | jvm-exception            | JVM custom exception thrown         | Metrics, Logs
    SERVICE        | jvm-gc                   | JVM Garbage Collection triggered    | Metrics, Logs
    SERVICE        | jvm-latency              | JVM method latency injection        | Metrics, Logs
    SERVICE        | jvm-cpu-stress           | JVM-specific CPU stress             | Metrics, Logs
    SERVICE        | dns-error                | DNS resolution failure              | Metrics, Traces, Logs
    NODE           | node_cpu                 | Node CPU stress                     | Metrics
    NODE           | node_disk                | Node disk/IO fault                  | Metrics
    NODE           | node_network_loss        | Node network packet loss            | Metrics
    NODE           | node_network_delay       | Node network latency                | Metrics
    SERVICE        | target_port_misconfig    | Service port misconfiguration       | Metrics, Traces, Logs
    SERVICE        | erroneous-code           | Application logic error/bug         | Metrics, Traces, Logs
    SERVICE        | io-fault                 | File system Read/Write error        | Metrics, Logs

    6) Final Output Format (STRICT)
    Return ONLY one JSON object (no markdown, no extra text, no tool traces, no intermediate reasoning).
    - Up to 3 root causes.
    - "location" MUST be a concrete instance name at the {instance_type} level.


    Hard constraints:
    - The response MUST start with "{" as the first character and end with "}" as the last character.
    - Output MUST be a JSON OBJECT, not a JSON string.
    - DO NOT wrap the JSON with quotes (no leading/trailing " or ').
    - DO NOT escape quotes inside JSON (no \" anywhere).
    - DO NOT include literal "\n" or "\\n" escape sequences; write normal newlines if needed.
    - DO NOT output markdown, code fences, YAML, XML, or any surrounding text.
    - DO NOT output explanations, tool traces, thoughts, or prefixes/suffixes of any kind.

    Schema (must match exactly):
    {{
    "anomaly type": "<anomaly type>",
    "root cause": [
        {{"location": "<instance_name>", "reason": "<simple explanation>"}},
        ...
    ]
    }}

    The response must start with "{" and end with "}" with no surrounding text.
    
    Self-check before sending:
    - If you are about to output anything other than a JSON object, STOP and output ONLY the JSON object.
    - Verify your output does NOT contain \" and does NOT start with a quote.
    """


def build_user_message(start_time, end_time, instace_type="service"):
    return f"A fault occurred from  {start_time} to {end_time} in {instace_type}. Please locate the accurate issue root cause."


# def build_project_details(
#     workspace, region, sls_project, logstore, metircstore, tracestore
# ):
#     return f"""Your UModel workspace is '{workspace}' in region '{region}', and the SLS project is '{sls_project}'.
#     The logstore is '{logstore}', the metricstore is '{metircstore}', the tracestore is '{tracestore}'.
#     Use this information when configuring your data source connections.
#     """


## MCP Agent Execution
async def run_mcp_only(
    start_time,
    end_time,
    instance_type="service",
    sls_endpoints="cn-heyuan=cn-heyuan.log.aliyuncs.com",
    cms_endpoints="cn-heyuan=metrics.cn-heyuan.aliyuncs.com",
    ground_truth=None,
    uuid=None,
    delay=201 * 24 * 60,
):
    prompt_start_time = _beijing_to_unix_seconds(
        _convert_to_beijing(start_time, delay=delay)
    )
    prompt_end_time = _beijing_to_unix_seconds(
        _convert_to_beijing(end_time, delay=delay)
    )
    system_prompt = build_system_prompt(
        prompt_start_time, prompt_end_time, instance_type
    )
    user_message = build_user_message(prompt_start_time, prompt_end_time, instance_type)
    project_details = mcp_tools.build_project_details(
        workspace="zy-aiops-challenges-2025",
        region="cn-heyuan",
        sls_project="default-cms-1102382765107602-cn-heyuan",
        logstore="aiops-dataset-logs",
        metircstore="aiops-dataset-metrics",
        tracestore="aiops-dataset-traces",
    )
    # mcp_query = f"{system_prompt}\n{project_details}\nUser Request:\n{user_message}\n"

    # python_executable = sys.executable  # stdio mode need python executable

    # access_key_id = os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_ID")
    # access_key_secret = os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_SECRET")

    # mcp_result_text = await run_mcp_agent(
    #     query=mcp_query,
    #     connection_mode="stdio",
    #     url_or_cmd=python_executable,
    #     access_key_id=access_key_id,
    #     access_key_secret=access_key_secret,
    #     # sls_endpoints=sls_endpoints if sls_endpoints else "cn-heyuan=default-cms-1102382765107602-cn-heyuan",
    #     # cms_endpoints=cms_endpoints if cms_endpoints else "cn-heyuan=default-cms-1102382765107602-cn-heyuan",
    #     sls_endpoints=sls_endpoints,
    #     cms_endpoints=cms_endpoints,
    # )


    mcp_result_text = await run_mcp_agent(
        system_prompt=system_prompt,
        project_details=project_details,
        user_prompt=user_message,
        connection_mode="sse",
        url="http://127.0.0.1:8000/sse",
    )

    mcp_result = parse_rca_json_output(mcp_result_text)

    if uuid:
        mcp_result["uuid"] = uuid
    mcp_result["start_time"] = start_time
    mcp_result["end_time"] = end_time
    mcp_result["instance_type"] = instance_type
    if ground_truth:
        mcp_result["ground_truth"] = ground_truth
    return mcp_result


## Local RCA Agent Execution
def run_rca_only(
    start_time,
    end_time,
    uuid=None,
    instance_type="service",
    ground_truth=None,
):
    tools = [
        traditional_tools.guide_intro,
        traditional_tools.analyze_fault_type,
        traditional_tools.detect_metrics,
        traditional_tools.detect_traces,
        traditional_tools.detect_logs,
        traditional_tools.get_system_info,
    ]
    system_prompt = build_system_prompt(start_time, end_time, instance_type)
    user_message = build_user_message(start_time, end_time, instance_type)
    rca_result = run_traditional_agent(system_prompt, user_message, tools)

    rca_result = parse_rca_json_output(rca_result)

    if uuid:
        rca_result["uuid"] = uuid
    if instance_type:
        rca_result["instance_type"] = instance_type
    rca_result["start_time"] = start_time
    rca_result["end_time"] = end_time
    if ground_truth:
        rca_result["ground_truth"] = ground_truth
    return rca_result


# async def run_comparison(
#     workspace,
#     region,
#     project,
#     start_time,
#     end_time,
#     sls_endpoints=None,
#     cms_endpoints=None,
# ):
#     print("=" * 60)
#     print("STARTING AGENT COMPARISON")
#     print("=" * 60)
#     print(f"Time Range: {start_time} to {end_time}")
#     print("=" * 60)

#     # --- 1. Run MCP Agent ---
#     print("\n" + "-" * 20 + " Running MCP Agent " + "-" * 20 + "\n")
#     mcp_result = await run_mcp_only(
#         start_time=start_time,
#         end_time=end_time,
#         sls_endpoints=sls_endpoints,
#         cms_endpoints=cms_endpoints,
#     )

#     # --- 2. Run Local RCA Agent ---
#     print("\n" + "-" * 20 + " Running Local RCA Agent " + "-" * 20 + "\n")
#     try:
#         rca_result = run_rca_only(
#             start_time=start_time,
#             end_time=end_time,
#         )
#     except Exception as e:
#         rca_result = {"error": str(e)}

#     # --- 3. Compare Results ---
#     print("\n" + "=" * 60)
#     print("COMPARISON RESULT")
#     print("=" * 60)

#     print("\n--- MCP Agent Output (JSON) ---")
#     print(json.dumps(mcp_result, indent=2, ensure_ascii=False))

#     print("\n--- Local RCA Agent Output (JSON) ---")
#     print(json.dumps(rca_result, indent=2, ensure_ascii=False))

#     # Save comparison to file
#     timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
#     result_dir = os.path.join(
#         os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "result"
#     )
#     if not os.path.exists(result_dir):
#         os.makedirs(result_dir)

#     filename = f"comparison_{timestamp}.txt"
#     filepath = os.path.join(result_dir, filename)

#     with open(filepath, "w", encoding="utf-8") as f:
#         f.write("=== MCP Agent Output ===\n")
#         f.write(json.dumps(mcp_result, indent=2, ensure_ascii=False) + "\n\n")
#         f.write("=== Local RCA Agent Output ===\n")
#         f.write(json.dumps(rca_result, indent=2, ensure_ascii=False) + "\n")

#     print(f"\nResults saved to {filepath}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Compare MCP Agent and Local RCA Agent"
    )
    # parser.add_argument("--start-time", default="2025-06-05T16:10:02Z", help="Start time in ISO format")
    # parser.add_argument("--end-time", default="2025-06-05T16:31:02Z", help="End time in ISO format")
    # parser.add_argument("--task", default="please locate the issue root cause", help="Description of the problem")
    # parser.add_argument(
    #     "--sls-endpoints", help="Override SLS endpoints (e.g. 'cn-region=host')"
    # )
    # parser.add_argument(
    #     "--cms-endpoints", help="Override CMS endpoints (e.g. 'cn-region=host')"
    # )
    parser.add_argument(
        "--mode",
        choices=["mcp", "rca", "both"],
        default="mcp",
        # default="rca",
        help="Run MCP agent, local RCA agent, or both",
    )

    parser.add_argument(
        "--fromIndex", type=int, default=0, help="Continue from a specific case index"
    )

    parser.add_argument(
        "--endIndex", type=int, default=None, help="End at a specific case index"
    )

    args = parser.parse_args()
    result_answers = []
    try:
        with open(os.path.join("data", "label.json"), "r", encoding="utf-8") as f:
            labels = json.load(f)
            labels_tmp = labels[
                args.fromIndex : (
                    args.endIndex if args.endIndex is not None else len(labels)
                )
            ]  # Continue from a specific case index

        for case in tqdm(labels_tmp, desc="Processing Cases", total=len(labels_tmp)):
            start_time = case["start_time"]
            end_time = case["end_time"]

            if args.mode == "mcp":
                result = asyncio.run(
                    run_mcp_only(
                        uuid=case.get("uuid"),
                        start_time=start_time,
                        end_time=end_time,
                        instance_type=case.get("instance_type"),
                        ground_truth=case.get("instance"),
                    )
                )
                result_answers.append(json.dumps(result, indent=2, ensure_ascii=False))
                print(json.dumps(result, indent=2, ensure_ascii=False))
            elif args.mode == "rca":
                result = run_rca_only(
                    uuid=case.get("uuid"),
                    start_time=start_time,
                    end_time=end_time,
                    instance_type=case.get("instance_type"),
                    ground_truth=case.get("instance"),
                )
                result_answers.append(json.dumps(result, indent=2, ensure_ascii=False))
                print(json.dumps(result, indent=2, ensure_ascii=False))
            else:
                pass

        # Save all results to a single file
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        result_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "result"
        )
        if not os.path.exists(result_dir):
            os.makedirs(result_dir)
        filename = f"{args.mode}_results_{timestamp}.jsonl"
        filepath = os.path.join(result_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            for answer in result_answers:
                f.write(answer + "\n\n")
        print(f"\nAll results saved to {filepath}")

    except Exception as e:
        print(f"An error occurred during processing: {e}")
